{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "from dateutil.parser import isoparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of all collections\n",
    "\n",
    "url = 'https://www.loc.gov/collections?fo=json&at=results,pagination&c=100'\n",
    "def paginate(url,collections=[]):\n",
    "    print('\\n'+url)\n",
    "    print('Starting api request beginning at: '+ str(time.ctime(time.time())))\n",
    "    response = requests.get(url)\n",
    "    print('Done. Processing api request beginning at: '+ str(time.ctime(time.time())))\n",
    "    if response.status_code == 429:\n",
    "        return response.headers\n",
    "        sys.exit()\n",
    "    else:\n",
    "        results = response.json()['results']\n",
    "        for result in results:\n",
    "            result_dict = {}\n",
    "            result_dict['on'] = result['items']+'?fo=json&at=results'\n",
    "            result_dict['title'] = result['title']\n",
    "            collections.append(result_dict)\n",
    "        pagination = response.json()['pagination']\n",
    "        if pagination['next']:\n",
    "            print('There is another page at: '+str(pagination['next']))\n",
    "            next_url = pagination['next']\n",
    "            time.sleep(10)\n",
    "            paginate(next_url,collections)\n",
    "        return collections\n",
    "\n",
    "collections = paginate(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df of collections\n",
    "\n",
    "partof_pd = pd.DataFrame(collections)\n",
    "\n",
    "#Get list of collection searches and titles\n",
    "partof_urls = partof_pd['on'].to_list()\n",
    "partof_titles = partof_pd['title'].to_list()\n",
    "partof_urls = [url.replace(\n",
    "    '?fo=json',\n",
    "    '?fa=original-format!:web+page|original-format!:event|original-format!:collection&fo=json'\n",
    "    ) for url in partof_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partof_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sample record from each collection\n",
    "#Sample record should be result #10 (11th result)\n",
    "sample_items=[]\n",
    "exclude = ['event','web page','collection','catalog']\n",
    "for url in partof_urls:\n",
    "    time.sleep(10)\n",
    "    record_num=3\n",
    "    finished = False\n",
    "    print('Checking collection: ' + url)\n",
    "    page1 = requests.get(url, params = {'c':100})\n",
    "    url_fixed = url.replace(\n",
    "        '?fa=original-format!:web+page|original-format!:event|original-format!:collection&fo=json',\n",
    "        '?fo=json'\n",
    "        )\n",
    "    try:\n",
    "        #Search until you find a record that's not an excluded format.\n",
    "        #If one is not found in records #10 - 100, then a sample isn't \n",
    "        # pulled from this collection.\n",
    "        while (record_num < 100) & (finished == False):\n",
    "            sample_item = None\n",
    "            test = page1.json()['results'][record_num]\n",
    "            print('Reviewing item: '+ str(test['title']))\n",
    "            print('Original format(s): '+ str(test['original_format']))\n",
    "            #If the record is an excluded format, try again\n",
    "            if any(original_format in exclude for original_format in test['original_format']):\n",
    "                finished=False\n",
    "                record_num += 1\n",
    "                print('Not using this record, wrong format.')\n",
    "            else:\n",
    "                sample_item = page1.json()['results'][record_num]\n",
    "                sample_item['partof_url'] = url_fixed\n",
    "                finished=True\n",
    "                print('Using this record! Moving on to next collection.\\n')\n",
    "    except:\n",
    "        sample_item = None\n",
    "        print('Something went wrong. Skipping this collection.\\n')\n",
    "    sample_items.append(sample_item)\n",
    "\n",
    "#return partof_urls\n",
    "partof_urls = [url.replace(\n",
    "    '|original-format!:web+page|original-format!:event|original-format!:collection&fo=json',\n",
    "    '&fo=json'\n",
    "    ) for url in partof_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = sample_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return partof_urls\n",
    "'''partof_urls = [url.replace(\n",
    "    '?fa=original-format!:web+page|original-format!:event|original-format!:collection&fo=json',\n",
    "    '?fo=json'\n",
    "    ) for url in partof_urls]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '.')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '.')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_sample_items = []\n",
    "for sample in sample_items:\n",
    "    flattened_sample = flatten_json(sample)\n",
    "    flattened_sample_items.append(flattened_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_sample_items_pd = pd.DataFrame(flattened_sample_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partof_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partof_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop blank rows created by collections with no sample items\n",
    "\n",
    "flattened_sample_items_pd.dropna(axis=0, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = flattened_sample_items_pd.columns.values.tolist()\n",
    "pattern = re.compile(r'^(.+)\\.\\d+$')\n",
    "bases_checked = []\n",
    "all_col_metadata = []\n",
    "\n",
    "#For each column\n",
    "for col in cols:\n",
    "    col_metadata = {}\n",
    "    types = []\n",
    "\n",
    "    #If the column ends in a number (is a list column, split up)    \n",
    "    if pattern.match(col):\n",
    "        base = re.match(r'^(.+)\\.\\d+$', col)[1]\n",
    "        #If this split-up column hasn't been reviewed yet\n",
    "        if base not in bases_checked:\n",
    "            bases_checked.append(base) #mark as checked\n",
    "            matches = []\n",
    "            sample_values=[]\n",
    "            #Find all the columns in this group\n",
    "            for other_col in cols: \n",
    "                if re.match(base+r'\\.\\d+', other_col):\n",
    "                    matches.append(other_col)\n",
    "                    #Make a list of the value types found in this group\n",
    "                    types_per_instance = [x.__name__ for x in flattened_sample_items_pd[other_col].dropna().apply(type).unique()]\n",
    "                    types.extend(types_per_instance)\n",
    "            #For each collection\n",
    "            for collection in partof_urls:\n",
    "                sample_values_per_collection = {}\n",
    "                sample_values_at_collection = []\n",
    "                #Get collection title based on collection url\n",
    "                collection_title = partof_pd[partof_pd['on']==collection]['title'].to_list()[0]\n",
    "                sample_values_per_collection['collection'] = collection_title\n",
    "                #Get sample values from each column in the group\n",
    "                for other_col in cols:\n",
    "                    if re.match(base+r'\\.\\d+', other_col):\n",
    "                        try:\n",
    "                            sample_value = flattened_sample_items_pd[flattened_sample_items_pd['partof_url'] == collection][other_col].to_list()[0]\n",
    "                            sample_values_at_collection.append(sample_value)\n",
    "                        except:\n",
    "                            pass\n",
    "                sample_values_per_collection['samples'] = sample_values_at_collection\n",
    "                sample_values.append(sample_values_per_collection)\n",
    "                drop_nans = [x for x in sample_values_at_collection if str(x) != 'nan']\n",
    "                if len(drop_nans)>0:\n",
    "                    col_metadata[collection_title] = random.choice(drop_nans)\n",
    "                else:\n",
    "                    col_metadata[collection_title] = None\n",
    "            \n",
    "            \n",
    "            #col_metadata['sample_values'] = sample_values\n",
    "    \n",
    "            col_metadata['types'] = list(set(types))\n",
    "            col_metadata['field'] = base.replace('.',' > ')\n",
    "            col_metadata['list'] = True\n",
    "            col_metadata['max_values_in_sample'] = len(matches)\n",
    "            col_metadata['cols'] = matches\n",
    "            \n",
    "            #Calculate how often the field is used vs. blank in collections\n",
    "            empty = flattened_sample_items_pd[base+'.0'].isna().sum()\n",
    "            col_metadata['used_in'] = len(flattened_sample_items_pd) - empty\n",
    "            col_metadata['used_in_percent'] = (len(flattened_sample_items_pd) - empty)*100/len(flattened_sample_items_pd)\n",
    "        \n",
    "            \n",
    "            all_col_metadata.append(col_metadata)\n",
    "            \n",
    "    else:\n",
    "        col_metadata['field'] = col.replace('.',' > ')\n",
    "        col_metadata['list'] = False\n",
    "        col_metadata['max_values_in_sample'] = 1\n",
    "        col_metadata['cols'] = col\n",
    "        col_metadata['types'] = [x.__name__ for x in flattened_sample_items_pd[col].dropna().apply(type).unique()]\n",
    "        empty = flattened_sample_items_pd[col].isna().sum()\n",
    "        col_metadata['used_in'] = len(flattened_sample_items_pd) - empty\n",
    "        col_metadata['used_in_percent'] = (len(flattened_sample_items_pd) - empty)*100/len(flattened_sample_items_pd)\n",
    "        \n",
    "\n",
    "        for collection in partof_urls:\n",
    "            try: #collections without any sample items won't work\n",
    "                collection_title = partof_pd[partof_pd['on']==collection]['title'].to_list()[0]\n",
    "                sample_value = flattened_sample_items_pd[flattened_sample_items_pd['partof_url'] == collection][col].to_list()[0]\n",
    "                col_metadata[collection_title] = sample_value \n",
    "            except:\n",
    "                pass\n",
    "        all_col_metadata.append(col_metadata)\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "all_fields = pd.DataFrame(all_col_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder the columns, to put the basic metadata in front and the collection sample values after\n",
    "move_to_front = ['field','used_in','used_in_percent','types','list','max_values_in_sample','cols']\n",
    "popoff = all_fields[move_to_front].copy()\n",
    "remainder = all_fields.drop(move_to_front, axis=1)\n",
    "move_to_back = remainder.columns.to_list()\n",
    "new_order = move_to_front + move_to_back\n",
    "all_fields = all_fields[new_order].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For repeat fields that have lists mid-way through the field hierarchy, drop all instances after the first one.\n",
    "# Drop any row where the field name has any digit between 1 and 9 (e.g., 10, 3, 27)\n",
    "all_fields = all_fields[all_fields['field'].str.contains(r'[1-9]')==False].copy()\n",
    "all_fields = all_fields[all_fields['field']!='partof_url'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the list of all the matching column groupings\n",
    "all_fields.drop('cols', axis=1, inplace=True)\n",
    "#Skip the field \"partof_url\". That was just for processing purposes\n",
    "all_fields = all_fields[all_fields['field']!='partof_url'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataTables errors if a column has a period or apostrophe. \n",
    "# Replace periods with spaces. Remove apostrophes\n",
    "all_fields.columns = all_fields.columns.str.replace(\".\", \" \")\n",
    "all_fields.columns = all_fields.columns.str.replace(\"\\'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any rows that have a blank field name column\n",
    "all_fields = all_fields[all_fields['field'].str.contains(r'^$')==False].copy()\n",
    "\n",
    "#Drop any collection columns that aren't loc.gov collections\n",
    "not_locgov = partof_pd[partof_pd['on'].str.contains('www.loc.gov')==False]['title'].to_list()\n",
    "not_locgov = [x.replace('.', ' ') for x in not_locgov]\n",
    "all_fields.drop(not_locgov, axis=1, inplace=True)\n",
    "\n",
    "#Sort rows (datatable also sorts for you, so could skip this)\n",
    "all_fields = all_fields.sort_values(by=['used_in','field','max_values_in_sample'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define patterns to check for\n",
    "url_pattern = re.compile(r'^http')\n",
    "integer = re.compile(r'^\\d+$')\n",
    "decimal = re.compile(r'^\\d+\\.\\d+$')\n",
    "#checking for timestamp is a little more complex:\n",
    "def is_date(string):\n",
    "    try: \n",
    "        isoparse(string)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "#For all rows, get a list of value types from all collection samples\n",
    "types = []\n",
    "i = 0\n",
    "for index, row in all_fields.iterrows():\n",
    "    format_types = []\n",
    "    #If it's already recognized as boolean, move on to next row.\n",
    "    if 'bool' in row['types']:\n",
    "        format_types.append('bool')\n",
    "    else:\n",
    "        for column in row[6:]:\n",
    "            if pd.isna(column):\n",
    "                continue\n",
    "            elif bool(url_pattern.match(str(column))):\n",
    "                format_types.append('URL')\n",
    "            elif is_date(str(column)):\n",
    "                format_types.append('iso timestamp')\n",
    "            elif bool(integer.match(str(column))):\n",
    "                format_types.append('int')\n",
    "            elif bool(decimal.match(str(column))):\n",
    "                format_types.append('decimal number')\n",
    "            else:\n",
    "                format_types.append('str')\n",
    "\n",
    "            \n",
    "    format_types = list(set(format_types))\n",
    "    types.append(format_types)\n",
    "\n",
    "print('These should be equal before replacing the types column:')\n",
    "print(len(types))\n",
    "print(len(all_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the types column with more detailed types\n",
    "all_fields['types'] = types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to JSON file\n",
    "all_fields.to_json('loc_fields.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For use in html page\n",
    "for field in all_fields.columns.to_list():\n",
    "    print('{ \\'data\\': \\''+field+'\\'},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For use in html page\n",
    "for field in all_fields.columns.to_list():\n",
    "    print('<th>'+field+'</th>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_fields.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
